{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uduaagAPFqb",
        "outputId": "89fd5b29-46c9-4d65-c1b3-d34dbae168b4"
      },
      "outputs": [],
      "source": [
        "# --- üì¶ SETUP ---\n",
        "!pip install icrawler pillow tqdm\n",
        "\n",
        "import os\n",
        "import random\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- üóÇÔ∏è Mount Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1YOylvvQjIj",
        "outputId": "9282ee92-3d36-41fa-9af1-5669e7384cb1"
      },
      "outputs": [],
      "source": [
        "!pip install simple_image_download pillow tqdm\n",
        "\n",
        "import os\n",
        "import random\n",
        "from simple_image_download import simple_image_download as simp\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWoIye8tPL0s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQb_mx1cbzDY"
      },
      "source": [
        "detect the ones that didnt do too hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX0TnIembyuR",
        "outputId": "4c7519c9-b2cb-4bc3-b393-d88b49da2a9f"
      },
      "outputs": [],
      "source": [
        "#!pip install icrawler pillow tqdm --quiet\n",
        "\n",
        "import os, json, random\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from icrawler.builtin import GoogleImageCrawler, BingImageCrawler\n",
        "\n",
        "# === Paths ===\n",
        "BASE_DIR = '/content/drive/MyDrive/food-101_data/food-101'\n",
        "IMAGES_DIR = os.path.join(BASE_DIR, 'images')\n",
        "META_DIR = os.path.join(BASE_DIR, 'meta')\n",
        "JSON_PATH = os.path.join(META_DIR, 'foods_by_cuisine.json')\n",
        "\n",
        "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "os.makedirs(META_DIR, exist_ok=True)\n",
        "\n",
        "# === Egyptian foods to exclude ===\n",
        "egyptian_foods = [\n",
        "    \"koshari\", \"molokhia\", \"mahshi\", \"hawawshi\", \"bamia\", \"ful medames\",\n",
        "    \"falafel\", \"feteer meshaltet\", \"sayadiya\", \"alexandrian liver sandwich\",\n",
        "    \"grilled kofta\", \"stuffed pigeon\", \"shawarma\", \"roz meammar\", \"fish tagine\",\n",
        "    \"okra stew\", \"egyptian salad\", \"tahini salad\", \"pickled eggplant\",\n",
        "    \"lamb fattah\", \"chicken fattah\", \"bechamel pasta\", \"sambousek\",\n",
        "    \"egyptian lentil soup\", \"white beans stew\", \"bessara\", \"torly\", \"molokhia with rabbit\",\n",
        "    \"chicken pane\", \"chicken shawarma\", \"baba ganoush\", \"chicken liver\", \"stuffed grape leaves\",\n",
        "    \"chicken molokhia\", \"fried tilapia\", \"grilled fish egyptian style\",\n",
        "    \"sayadiya rice\", \"macarona bechamel\", \"basbousa\", \"kunafa\", \"baklava\", \"roz bel laban\",\n",
        "    \"om ali\", \"qatayef\", \"kahk\", \"goulash dessert\", \"rice pudding egyptian\"\n",
        "]\n",
        "\n",
        "# === Load JSON and filter non-Egyptian cuisines ===\n",
        "with open(JSON_PATH, 'r') as f:\n",
        "    foods_by_cuisine = json.load(f)\n",
        "exclude_until = \"french\"\n",
        "keys = list(foods_by_cuisine.keys())\n",
        "\n",
        "# Find the index of \"french\"\n",
        "cut_index = keys.index(exclude_until.lower()) + 1\n",
        "\n",
        "# Keep everything after \"french\"\n",
        "non_egyptian = {k: foods_by_cuisine[k] for k in keys[cut_index:]}\n",
        "\n",
        "#non_egyptian = {k: v for k, v in foods_by_cuisine.items() if k.lower() != (\"egyptian\")}\n",
        "\n",
        "# Save filtered version (optional)\n",
        "filtered_path = os.path.join(META_DIR, 'foods_non_egyptian.json')\n",
        "with open(filtered_path, 'w') as f:\n",
        "    json.dump(non_egyptian, f, indent=2, ensure_ascii=False)\n",
        "print(f\"‚úÖ Saved non-Egyptian cuisines to: {filtered_path}\")\n",
        "\n",
        "# === Utility: clean and resize images ===\n",
        "def clean_and_resize(folder):\n",
        "    if not os.path.exists(folder): return\n",
        "    for f in os.listdir(folder):\n",
        "        path = os.path.join(folder, f)\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img = img.resize((512, 512))\n",
        "            img.save(path)\n",
        "        except Exception:\n",
        "            os.remove(path)\n",
        "\n",
        "# === Utility: image downloader ===\n",
        "def download_images(cuisine, dish, max_num=100):\n",
        "    safe_dish = dish.replace(\" \", \"_\")\n",
        "    save_dir = os.path.join(IMAGES_DIR, cuisine, safe_dish)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Skip if enough images exist\n",
        "    existing = len([f for f in os.listdir(save_dir) if f.endswith(('.jpg', '.png'))])\n",
        "    if existing >= 50:\n",
        "        return\n",
        "\n",
        "    print(f\"üü¢ {cuisine} ‚Üí {dish}: downloading images...\")\n",
        "\n",
        "    search_terms = [\n",
        "        f\"{dish} {cuisine} food\",\n",
        "        f\"{dish} {cuisine} cuisine\",\n",
        "        f\"{dish} traditional {cuisine}\",\n",
        "        f\"{dish} meal\"\n",
        "    ]\n",
        "\n",
        "    total_downloaded = 0\n",
        "    for query in search_terms:\n",
        "        if total_downloaded >= max_num:\n",
        "            break\n",
        "        for crawler_cls in [GoogleImageCrawler, BingImageCrawler]:\n",
        "            crawler = crawler_cls(storage={'root_dir': save_dir})\n",
        "            try:\n",
        "                crawler.crawl(keyword=query, max_num=max_num // len(search_terms), file_idx_offset=total_downloaded)\n",
        "                total_downloaded = len(os.listdir(save_dir))\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è {crawler_cls.__name__} failed for {query}: {e}\")\n",
        "    clean_and_resize(save_dir)\n",
        "    print(f\"üì∏ Done: {len(os.listdir(save_dir))} images saved for {dish}\")\n",
        "\n",
        "# === Download all non-Egyptian dishes ===\n",
        "classes = []\n",
        "for cuisine, dishes in non_egyptian.items():\n",
        "    print(f\"\\nüçΩÔ∏è Processing cuisine: {cuisine}\")\n",
        "    for dish in tqdm(dishes):\n",
        "        dish_lower = dish.lower()\n",
        "        # Skip dishes that appear Egyptian by name\n",
        "        if any(e.lower() in dish_lower for e in egyptian_foods):\n",
        "            continue\n",
        "\n",
        "        download_images(cuisine, dish)\n",
        "        classes.append(f\"{cuisine}/{dish.replace(' ', '_')}\")\n",
        "\n",
        "# === Generate Food-101 meta files ===\n",
        "print(\"\\nüßæ Generating meta files...\")\n",
        "classes_path = os.path.join(META_DIR, \"classes.txt\")\n",
        "train_path = os.path.join(META_DIR, \"train.txt\")\n",
        "test_path = os.path.join(META_DIR, \"test.txt\")\n",
        "\n",
        "# Write classes.txt\n",
        "with open(classes_path, \"w\") as f:\n",
        "    for c in classes:\n",
        "        f.write(c + \"\\n\")\n",
        "\n",
        "# Build train/test split\n",
        "train_lines, test_lines = [], []\n",
        "for c in classes:\n",
        "    cuisine, dish = c.split(\"/\")\n",
        "    folder = os.path.join(IMAGES_DIR, cuisine, dish)\n",
        "    imgs = [f\"{c}/{img}\" for img in os.listdir(folder) if img.endswith(('.jpg', '.png'))]\n",
        "    if len(imgs) < 5:\n",
        "        continue  # skip underpopulated classes\n",
        "    random.shuffle(imgs)\n",
        "    split = int(len(imgs) * 0.8)\n",
        "    train_lines += imgs[:split]\n",
        "    test_lines += imgs[split:]\n",
        "\n",
        "with open(train_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(train_lines))\n",
        "with open(test_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(test_lines))\n",
        "\n",
        "print(f\"\\n‚úÖ All done!\")\n",
        "print(f\"Total cuisines: {len(non_egyptian)}\")\n",
        "print(f\"Total classes: {len(classes)}\")\n",
        "print(f\"Train samples: {len(train_lines)}, Test samples: {len(test_lines)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnoGI5KMgjtb",
        "outputId": "7f7e0d97-9791-4e4f-d14e-ba5af21f3aa1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# === Base paths ===\n",
        "BASE_DIR = '/content/drive/MyDrive/food-101_data/food-101'\n",
        "IMAGES_DIR = os.path.join(BASE_DIR, 'images')\n",
        "META_DIR = os.path.join(BASE_DIR, 'meta')\n",
        "os.makedirs(META_DIR, exist_ok=True)\n",
        "\n",
        "# === Egyptian foods to add ===\n",
        "egyptian_foods = [\n",
        "    \"koshari\", \"molokhia\", \"mahshi\", \"hawawshi\", \"bamia\", \"ful_medames\",\n",
        "    \"falafel\", \"feteer_meshaltet\", \"sayadiya\", \"alexandrian_liver_sandwich\",\n",
        "    \"grilled_kofta\", \"stuffed_pigeon\", \"shawarma\", \"roz_meammar\",\n",
        "    \"fish_tagine\", \"okra_stew\", \"egyptian_salad\", \"tahini_salad\",\n",
        "    \"pickled_eggplant\", \"lamb_fattah\", \"chicken_fattah\", \"bechamel_pasta\",\n",
        "    \"sambousek\", \"egyptian_lentil_soup\", \"white_beans_stew\", \"bessara\",\n",
        "    \"torly\", \"molokhia_with_rabbit\", \"chicken_pane\", \"baba_ganoush\",\n",
        "    \"stuffed_grape_leaves\", \"fried_tilapia\", \"macarona_bechamel\",\n",
        "    \"basbousa\", \"kunafa\", \"roz_bel_laban\", \"om_ali\", \"qatayef\", \"kahk\"\n",
        "]\n",
        "\n",
        "# === Make sure each food directory exists ===\n",
        "valid_foods = []\n",
        "for food in egyptian_foods:\n",
        "    folder = os.path.join(IMAGES_DIR, food)\n",
        "    if os.path.exists(folder) and len(os.listdir(folder)) > 0:\n",
        "        valid_foods.append(food)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Skipping {food} (no images found)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Found {len(valid_foods)} Egyptian food classes with images.\\n\")\n",
        "\n",
        "# === Create train/test splits ===\n",
        "train_ratio = 0.8  # 80% train, 20% test\n",
        "train_split = []\n",
        "test_split = []\n",
        "\n",
        "for food in valid_foods:\n",
        "    img_dir = os.path.join(IMAGES_DIR, food)\n",
        "    imgs = [os.path.splitext(f)[0] for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n",
        "    random.shuffle(imgs)\n",
        "\n",
        "    split_idx = int(len(imgs) * train_ratio)\n",
        "    train_imgs = imgs[:split_idx]\n",
        "    test_imgs = imgs[split_idx:]\n",
        "\n",
        "    # Append paths in the Food-101 format (class_name/image_name)\n",
        "    for img in train_imgs:\n",
        "        train_split.append(f\"{food}/{img}\")\n",
        "    for img in test_imgs:\n",
        "        test_split.append(f\"{food}/{img}\")\n",
        "\n",
        "# === Save updated classes, train.txt, and test.txt ===\n",
        "classes_path = os.path.join(META_DIR, 'classes.txt')\n",
        "train_path = os.path.join(META_DIR, 'train.txt')\n",
        "test_path = os.path.join(META_DIR, 'test.txt')\n",
        "\n",
        "# Read existing classes if available\n",
        "if os.path.exists(classes_path):\n",
        "    with open(classes_path, 'r') as f:\n",
        "        existing_classes = [line.strip() for line in f.readlines()]\n",
        "else:\n",
        "    existing_classes = []\n",
        "\n",
        "# Add new ones if not already there\n",
        "for food in valid_foods:\n",
        "    if food not in existing_classes:\n",
        "        existing_classes.append(food)\n",
        "\n",
        "# Save updated class list\n",
        "with open(classes_path, 'w') as f:\n",
        "    f.write(\"\\n\".join(sorted(existing_classes)))\n",
        "\n",
        "# Merge with existing train/test if they exist\n",
        "def append_or_create(path, new_lines):\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'a') as f:\n",
        "            f.write(\"\\n\" + \"\\n\".join(new_lines))\n",
        "    else:\n",
        "        with open(path, 'w') as f:\n",
        "            f.write(\"\\n\".join(new_lines))\n",
        "\n",
        "append_or_create(train_path, train_split)\n",
        "append_or_create(test_path, test_split)\n",
        "\n",
        "print(\"‚úÖ Train/test splits and class list updated successfully!\")\n",
        "\n",
        "# === Optional: print summary ===\n",
        "print(f\"\\nüìÅ Classes total: {len(existing_classes)}\")\n",
        "print(f\"üß© Egyptian classes added: {len(valid_foods)}\")\n",
        "print(f\"üìò Train samples added: {len(train_split)}\")\n",
        "print(f\"üìó Test samples added: {len(test_split)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3eVrfydho6l",
        "outputId": "cd5d4f69-3739-48fb-a3c6-79718f341775"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "classes_path = os.path.join(META_DIR, 'classes.txt')\n",
        "\n",
        "# === Read classes.txt ===\n",
        "if os.path.exists(classes_path):\n",
        "    with open(classes_path, 'r') as f:\n",
        "        all_classes = [line.strip() for line in f.readlines()]\n",
        "else:\n",
        "    all_classes = []\n",
        "\n",
        "# === Collect folder names in /images ===\n",
        "image_folders = [d for d in os.listdir(IMAGES_DIR) if os.path.isdir(os.path.join(IMAGES_DIR, d))]\n",
        "\n",
        "# === Comparisons ===\n",
        "in_classes_not_egyptian = [c for c in all_classes if c not in egyptian_foods]\n",
        "in_egyptian_and_classes = [c for c in all_classes if c in egyptian_foods]\n",
        "in_classes_but_missing_folder = [c for c in all_classes if c not in image_folders]\n",
        "valid_foods_in_classes = [c for c in all_classes if c in valid_foods]\n",
        "\n",
        "# === Print results ===\n",
        "print(\"\\n================= DATASET CONSISTENCY REPORT =================\")\n",
        "\n",
        "print(f\"üìò Total classes in classes.txt: {len(all_classes)}\")\n",
        "print(f\"üá™üá¨ Egyptian classes (list): {len(egyptian_foods)}\")\n",
        "print(f\"üìÅ Valid Egyptian folders found: {len(valid_foods)}\\n\")\n",
        "\n",
        "print(\"‚úÖ Classes both in classes.txt and Egyptian list:\")\n",
        "for c in in_egyptian_and_classes:\n",
        "    print(f\"  - {c}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Classes in classes.txt but NOT in Egyptian list (original Food-101):\")\n",
        "for c in in_classes_not_egyptian[:20]:\n",
        "    print(f\"  - {c}\")\n",
        "if len(in_classes_not_egyptian) > 20:\n",
        "    print(f\"  ... and {len(in_classes_not_egyptian) - 20} more ...\")\n",
        "\n",
        "print(\"\\n‚ùå Classes listed in classes.txt but have NO image folder:\")\n",
        "for c in in_classes_but_missing_folder:\n",
        "    print(f\"  - {c}\")\n",
        "\n",
        "print(\"\\n‚úÖ Valid foods currently in both classes.txt and /images/:\")\n",
        "for c in valid_foods_in_classes:\n",
        "    print(f\"  - {c}\")\n",
        "\n",
        "print(\"\\n==============================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "JZhO6ILP4ydL",
        "outputId": "db85cf8a-987d-4fa6-f84a-44c8bac85144"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to your dataset root\n",
        "dataset_dir = '/content/drive/MyDrive/food-101_data/food-101'\n",
        "\n",
        "# --------------------------------------\n",
        "# 1Ô∏è‚É£ Collect all image paths and classes\n",
        "# --------------------------------------\n",
        "data = []\n",
        "classes = set()\n",
        "\n",
        "for cuisine in os.listdir(dataset_dir):\n",
        "    cuisine_path = os.path.join(dataset_dir, cuisine)\n",
        "    if not os.path.isdir(cuisine_path):\n",
        "        continue\n",
        "\n",
        "    for food_class in os.listdir(cuisine_path):\n",
        "        class_path = os.path.join(cuisine_path, food_class)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Register this class\n",
        "        classes.add(food_class)\n",
        "\n",
        "        # Collect all images under this class\n",
        "        for img in os.listdir(class_path):\n",
        "            if img.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")):\n",
        "                img_path = os.path.join(class_path, img)\n",
        "                data.append({\"path\": img_path, \"class\": food_class, \"cuisine\": cuisine})\n",
        "\n",
        "# Sort for consistency\n",
        "classes = sorted(list(classes))\n",
        "\n",
        "# --------------------------------------\n",
        "# 2Ô∏è‚É£ Create Train/Val/Test Splits\n",
        "# --------------------------------------\n",
        "# Shuffle for randomness\n",
        "random.shuffle(data)\n",
        "\n",
        "# 80% train, 10% val, 10% test\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# --------------------------------------\n",
        "# 3Ô∏è‚É£ Build metadata summary\n",
        "# --------------------------------------\n",
        "metadata = {\n",
        "    \"total_images\": len(data),\n",
        "    \"num_classes\": len(classes),\n",
        "    \"classes\": classes,\n",
        "    \"split\": {\n",
        "        \"train\": len(train_data),\n",
        "        \"val\": len(val_data),\n",
        "        \"test\": len(test_data)\n",
        "    },\n",
        "    \"example_structure\": {\n",
        "        \"path\": data[0][\"path\"] if data else None,\n",
        "        \"class\": data[0][\"class\"] if data else None,\n",
        "        \"cuisine\": data[0][\"cuisine\"] if data else None\n",
        "    }\n",
        "}\n",
        "\n",
        "# --------------------------------------\n",
        "# 4Ô∏è‚É£ Save metadata and splits\n",
        "# --------------------------------------\n",
        "output_dir = os.path.join(dataset_dir, \"metadata\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "with open(os.path.join(output_dir, \"train.json\"), \"w\") as f:\n",
        "    json.dump(train_data, f, indent=4)\n",
        "\n",
        "with open(os.path.join(output_dir, \"val.json\"), \"w\") as f:\n",
        "    json.dump(val_data, f, indent=4)\n",
        "\n",
        "with open(os.path.join(output_dir, \"test.json\"), \"w\") as f:\n",
        "    json.dump(test_data, f, indent=4)\n",
        "\n",
        "print(f\"‚úÖ Metadata and splits saved in {output_dir}\")\n",
        "print(f\"Classes found: {len(classes)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
