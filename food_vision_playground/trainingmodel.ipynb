{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ij8su90EwGH"
      },
      "outputs": [],
      "source": [
        "!cd /content/drive/MyDrive\n",
        "!mkdir -p NN\n",
        "!cd NN\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FatemaKotb/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image.git\n",
        "!cd An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ1ypTNmFLHv",
        "outputId": "f4c2d0a3-d209-4b39-a1af-654e3ce8e632"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 190 (delta 77), reused 151 (delta 40), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (190/190), 1.81 MiB | 12.96 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jMndxDXFRan",
        "outputId": "bf62ea7b-ab5b-4d91-b8dc-a3ffe385aa36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm opencv-python scipy matplotlib tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jMIW8_4FUzh",
        "outputId": "8e268bf7-fd01-413b-f36f-8d911a0633af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cpu)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "YI6IvmEuFfNB",
        "outputId": "170ce10d-a81e-4c3b-d42a-b4631a6e7e50"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3595646193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \"\"\"\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \"\"\"\n\u001b[0;32m--> 614\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct way to change directory in Colab\n",
        "%cd /content/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image\n",
        "\n",
        "# Confirm\n",
        "!pwd\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKM2ifp7F8eE",
        "outputId": "37943093-6870-44d5-e314-418d29e9df23"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image\n",
            "/content/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image\n",
            "clip_finetune  food_vision_playground  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEW_LYljKVPk",
        "outputId": "2d0a8771-bdb0-4d7c-9f54-98050af0f3bc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clip_finetune  food_vision_playground  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config user.name \"maryammmabdelrahman-eng\"\n",
        "!git config user.email \"maryammmabdelrahman@gmail.com\""
      ],
      "metadata": {
        "id": "F3SsFGInIK_s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -b maskedrcnn_clip_pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpnxXn2kMC2v",
        "outputId": "243bb843-ebe5-402b-bcef-43503352a827"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: A branch named 'maskedrcnn_clip_pipeline' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Install dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.1/index.html\n",
        "!pip install opencv-python scipy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxIinqnULR1",
        "outputId": "7a2a4b03-8f31-419f-9911-e6fb6c68d955"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.1/index.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement detectron2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for detectron2\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================\n",
        "# Create dataset directory in Drive\n",
        "# ===============================\n",
        "DATASET_DIR = \"/content/drive/MyDrive/datasets/FoodSeg103\"\n",
        "!mkdir -p \"$DATASET_DIR\"\n",
        "\n",
        "# ===============================\n",
        "# Download FoodSeg103-256 zip into Drive\n",
        "# ===============================\n",
        "!wget https://zenodo.org/record/15729578/files/FoodSeg103-256.zip \\\n",
        "    -O \"$DATASET_DIR/FoodSeg103-256.zip\"\n",
        "\n",
        "# ===============================\n",
        "# Unzip inside Drive\n",
        "# ===============================\n",
        "!unzip -qq \"$DATASET_DIR/FoodSeg103-256.zip\" -d \"$DATASET_DIR\"\n",
        "\n",
        "# ===============================\n",
        "# Verify contents\n",
        "# ===============================\n",
        "!ls \"$DATASET_DIR\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWL0or9NRRUd",
        "outputId": "90411eac-3f9f-4e70-a527-c30cb3c7a1a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-28 14:03:12--  https://zenodo.org/record/15729578/files/FoodSeg103-256.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.153, 188.185.48.75, 137.138.52.235, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/15729578/files/FoodSeg103-256.zip [following]\n",
            "--2025-12-28 14:03:14--  https://zenodo.org/records/15729578/files/FoodSeg103-256.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1199113322 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/datasets/FoodSeg103/FoodSeg103-256.zip’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   1.12G   821KB/s    in 26m 24s \n",
            "\n",
            "2025-12-28 14:29:38 (739 KB/s) - ‘/content/drive/MyDrive/datasets/FoodSeg103/FoodSeg103-256.zip’ saved [1199113322/1199113322]\n",
            "\n",
            "FoodSeg103-256\tFoodSeg103-256.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQCNZyYoWhx3",
        "outputId": "f5a9f9d7-4094-41b6-b8b8-3f792608f19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets  food_vision_playground  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tree if needed\n",
        "!apt-get install -qq tree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2htUnrAyYAjO",
        "outputId": "279f1b56-2ea3-4f69-f601-5eba3aa88a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show full tree of dataset\n",
        "!tree -L 3 /content/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image/datasets/FoodSeg103/FoodSeg103-256\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5lPQdBcYT8o",
        "outputId": "4c2ab48a-9423-4a7e-b455-b17a4ca08a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image/datasets/FoodSeg103/FoodSeg103-256\u001b[0m\n",
            "├── \u001b[00mcategory_id.txt\u001b[0m\n",
            "├── \u001b[01;34mImages\u001b[0m\n",
            "│   ├── \u001b[01;34mann_dir\u001b[0m\n",
            "│   │   ├── \u001b[01;34mtest\u001b[0m\n",
            "│   │   └── \u001b[01;34mtrain\u001b[0m\n",
            "│   └── \u001b[01;34mimg_dir\u001b[0m\n",
            "│       ├── \u001b[01;34mtest\u001b[0m\n",
            "│       └── \u001b[01;34mtrain\u001b[0m\n",
            "├── \u001b[01;34mImageSets\u001b[0m\n",
            "│   ├── \u001b[00mtest.txt\u001b[0m\n",
            "│   └── \u001b[00mtrain.txt\u001b[0m\n",
            "├── \u001b[01;34mInstanceMasks\u001b[0m\n",
            "├── \u001b[00mReadme.txt\u001b[0m\n",
            "├── \u001b[00mtest_recipe1m_id.txt\u001b[0m\n",
            "└── \u001b[00mtrain_test_recipe1m_id.txt\u001b[0m\n",
            "\n",
            "9 directories, 6 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Base dataset path\n",
        "DATA_DIR = DATASET_DIR\n",
        "# Image and annotation directories\n",
        "IMG_DIR_TRAIN = os.path.join(DATA_DIR, \"Images\", \"img_dir\", \"train\")\n",
        "IMG_DIR_TEST = os.path.join(DATA_DIR, \"Images\", \"img_dir\", \"test\")\n",
        "\n",
        "MASK_DIR_TRAIN = os.path.join(DATA_DIR, \"Images\", \"ann_dir\", \"train\")\n",
        "MASK_DIR_TEST = os.path.join(DATA_DIR, \"Images\", \"ann_dir\", \"test\")\n",
        "\n",
        "# Read train/test file lists\n",
        "train_files = [x.strip() for x in open(os.path.join(DATA_DIR, \"ImageSets\", \"train.txt\")).readlines()]\n",
        "test_files = [x.strip() for x in open(os.path.join(DATA_DIR, \"ImageSets\", \"test.txt\")).readlines()]\n",
        "\n",
        "# Strip .jpg from IDs\n",
        "train_files = [f.replace(\".jpg\", \"\") for f in train_files]\n",
        "test_files = [f.replace(\".jpg\", \"\") for f in test_files]\n",
        "\n",
        "# Sanity check\n",
        "print(\"Sample IDs from train.txt:\", train_files[:5])\n",
        "\n",
        "# Function to get full path of image/mask given a file ID\n",
        "def get_image_mask_paths(file_id, train=True):\n",
        "    img_dir = IMG_DIR_TRAIN if train else IMG_DIR_TEST\n",
        "    mask_dir = MASK_DIR_TRAIN if train else MASK_DIR_TEST\n",
        "    img_path = os.path.join(img_dir, f\"{file_id}.bmp\")\n",
        "    mask_path = os.path.join(mask_dir, f\"{file_id}.bmp\")\n",
        "    if not os.path.exists(img_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "    if not os.path.exists(mask_path):\n",
        "        raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
        "    return img_path, mask_path\n",
        "\n",
        "# Load a sample image/mask\n",
        "sample_id = train_files[0]  # e.g., \"00000000\"\n",
        "sample_img_path, sample_mask_path = get_image_mask_paths(sample_id, train=True)\n",
        "\n",
        "sample_img = Image.open(sample_img_path).convert(\"RGB\")\n",
        "sample_mask = Image.open(sample_mask_path)\n",
        "\n",
        "# Display sample image and mask\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(sample_img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(sample_mask)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kQXBteiQVGvJ",
        "outputId": "2fac1d7a-8c4c-4ff4-8074-47ea2b01b447"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/datasets/FoodSeg103/ImageSets/train.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2134480914.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Read train/test file lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ImageSets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ImageSets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/datasets/FoodSeg103/ImageSets/train.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.transforms import functional as F\n",
        "import numpy as np\n",
        "\n",
        "class FoodSegDataset(Dataset):\n",
        "    def __init__(self, img_ids, img_dir, mask_dir, transforms=None):\n",
        "        self.img_ids = img_ids\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.img_ids[idx]\n",
        "        img_path = os.path.join(self.img_dir, f\"{file_id}.bmp\")\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{file_id}.bmp\")\n",
        "\n",
        "        # Load image and mask\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        # Convert mask to numpy array\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # If the mask is multi-class, convert each instance into a separate binary mask\n",
        "        obj_ids = np.unique(mask)\n",
        "        obj_ids = obj_ids[obj_ids != 0]  # remove background\n",
        "\n",
        "        masks = mask == obj_ids[:, None, None]  # shape: [num_objs, H, W]\n",
        "\n",
        "        # Bounding boxes [x_min, y_min, x_max, y_max]\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)  # all 1s (food class)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": image_id,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        img = F.to_tensor(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = FoodSegDataset(\n",
        "    img_ids=train_files,\n",
        "    img_dir=IMG_DIR_TRAIN,\n",
        "    mask_dir=MASK_DIR_TRAIN\n",
        ")\n",
        "\n",
        "test_dataset = FoodSegDataset(\n",
        "    img_ids=test_files,\n",
        "    img_dir=IMG_DIR_TEST,\n",
        "    mask_dir=MASK_DIR_TEST\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Test a batch\n",
        "images, targets = next(iter(train_loader))\n",
        "print(\"Batch images:\", len(images))\n",
        "print(\"Batch targets:\", targets[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7-CPhUKzaqiP",
        "outputId": "acd99fd6-83d3-4f64-ed55-815e0a2a8bc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-530252270.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Create train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m train_dataset = FoodSegDataset(\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mimg_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mimg_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_DIR_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mmask_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMASK_DIR_TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset Setup\n",
        "# ----------------------------\n",
        "DATA_DIR = \"/content/drive/MyDrive/datasets/FoodSeg103/FoodSeg103-256\"\n",
        "\n",
        "IMG_DIR_TRAIN = os.path.join(DATA_DIR, \"Images\", \"img_dir\", \"train\")\n",
        "IMG_DIR_TEST = os.path.join(DATA_DIR, \"Images\", \"img_dir\", \"test\")\n",
        "MASK_DIR_TRAIN = os.path.join(DATA_DIR, \"Images\", \"ann_dir\", \"train\")\n",
        "MASK_DIR_TEST = os.path.join(DATA_DIR, \"Images\", \"ann_dir\", \"test\")\n",
        "\n",
        "train_files = [x.strip().replace(\".jpg\", \"\") for x in open(os.path.join(DATA_DIR, \"ImageSets\", \"train.txt\")).readlines()]\n",
        "test_files = [x.strip().replace(\".jpg\", \"\") for x in open(os.path.join(DATA_DIR, \"ImageSets\", \"test.txt\")).readlines()]\n",
        "\n",
        "print(\"Number of train images:\", len(train_files))\n",
        "print(\"Number of test images:\", len(test_files))\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset Class\n",
        "# ----------------------------\n",
        "class FoodSegDataset(Dataset):\n",
        "    def __init__(self, file_ids, img_dir, mask_dir, transforms=None):\n",
        "        self.file_ids = file_ids\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_ids[idx]\n",
        "        img_path = os.path.join(self.img_dir, f\"{file_id}.bmp\")\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{file_id}.bmp\")\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Convert mask to instance labels (1..N)\n",
        "        obj_ids = np.unique(mask)\n",
        "        obj_ids = obj_ids[obj_ids != 0]  # remove background\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        boxes = []\n",
        "        for m in masks:\n",
        "            pos = np.where(m)\n",
        "            xmin = int(np.min(pos[1]))\n",
        "            xmax = int(np.max(pos[1]))\n",
        "            ymin = int(np.min(pos[0]))\n",
        "            ymax = int(np.max(pos[0]))\n",
        "            # Ensure box is valid\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "            else:\n",
        "                boxes.append([0, 0, 1, 1])  # fallback\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((len(boxes),), dtype=torch.int64)  # one label per instance\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        else:\n",
        "            img = F.to_tensor(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# ----------------------------\n",
        "# DataLoaders\n",
        "# ----------------------------\n",
        "train_dataset = FoodSegDataset(train_files, IMG_DIR_TRAIN, MASK_DIR_TRAIN)\n",
        "test_dataset = FoodSegDataset(test_files, IMG_DIR_TEST, MASK_DIR_TEST)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "num_classes = 104  # 103 foods + background\n",
        "model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")  # pretrained on COCO\n",
        "\n",
        "# Replace the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Replace the mask predictor\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Optimizer & Scheduler\n",
        "# ----------------------------\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, targets in train_loader:\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(imgs, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Visualize after every 5 epochs\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        model.eval()\n",
        "        sample_img, _ = train_dataset[0]\n",
        "        with torch.no_grad():\n",
        "            prediction = model([sample_img.to(device)])\n",
        "        masks = prediction[0]['masks'].cpu().numpy()\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.imshow(sample_img.permute(1,2,0))\n",
        "        for i in range(min(3, masks.shape[0])):  # show first 3 masks\n",
        "            plt.imshow(masks[i,0], alpha=0.5)\n",
        "        plt.title(f\"Epoch {epoch+1} Sample Prediction\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Loss Curve\n",
        "# ----------------------------\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs+1), train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Testing (sample)\n",
        "# ----------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_img, _ = test_dataset[0]\n",
        "    prediction = model([sample_img.to(device)])\n",
        "    masks = prediction[0]['masks'].cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(sample_img.permute(1,2,0))\n",
        "for i in range(min(3, masks.shape[0])):\n",
        "    plt.imshow(masks[i,0], alpha=0.5)\n",
        "plt.title(\"Sample Test Prediction\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjNE3-6htry",
        "outputId": "9808d6c9-862f-47df-a180-522ac3bcf1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train images: 4983\n",
            "Number of test images: 2135\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 75.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Colab Script: Integrate RCNN into GitHub Pipeline\n",
        "# ----------------------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from scripts.factory import PipelineFactoryConfig, build_default_pipeline\n",
        "from scripts.run_pipeline import load_rgb, ensure_uint8_rgb, _setup_logger\n",
        "\n",
        "# ----------------------------\n",
        "# Paths\n",
        "# ----------------------------\n",
        "REPO_DIR = \"/content/An-Instance-Aware-Multi-Task-Network-for-Food-Calorie-Estimation-from-a-Single-Image\"\n",
        "MODEL_PATH = os.path.join(REPO_DIR, \"models\", \"rcnn_foodseg103.pth\")\n",
        "TEST_IMAGE = os.path.join(REPO_DIR, \"datasets/FoodSeg103/FoodSeg103-256/Images/img_dir/test/00000000.bmp\")\n",
        "\n",
        "os.makedirs(os.path.join(REPO_DIR, \"models\"), exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Save trained RCNN (already in memory as `model`)\n",
        "# ----------------------------\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(f\"Saved trained RCNN to {MODEL_PATH}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Edit factory.py to support custom RCNN\n",
        "# ----------------------------\n",
        "FACTORY_FILE = os.path.join(REPO_DIR, \"scripts/factory.py\")\n",
        "with open(FACTORY_FILE, \"r\") as f:\n",
        "    factory_code = f.read()\n",
        "\n",
        "if \"custom_seg_model\" not in factory_code:\n",
        "    factory_code = factory_code.replace(\n",
        "        \"def build_default_pipeline(cfg: PipelineFactoryConfig):\",\n",
        "        \"def build_default_pipeline(cfg: PipelineFactoryConfig, custom_seg_model=None):\"\n",
        "    ).replace(\n",
        "        \"seg_block = SegmentationBlock(cfg.device)\",\n",
        "        \"seg_block = custom_seg_model if custom_seg_model is not None else SegmentationBlock(cfg.device)\"\n",
        "    )\n",
        "\n",
        "with open(FACTORY_FILE, \"w\") as f:\n",
        "    f.write(factory_code)\n",
        "print(\"Edited factory.py to support custom RCNN\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Load RCNN and build pipeline\n",
        "# ----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "num_classes = 104\n",
        "rcnn_model = maskrcnn_resnet50_fpn(weights=None)\n",
        "in_features_box = rcnn_model.roi_heads.box_predictor.cls_score.in_features\n",
        "rcnn_model.roi_heads.box_predictor = torch.nn.Linear(in_features_box, num_classes)\n",
        "\n",
        "in_features_mask = rcnn_model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "rcnn_model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "\n",
        "rcnn_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "rcnn_model.to(device).eval()\n",
        "print(\"Loaded RCNN model for pipeline\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Build pipeline using trained RCNN\n",
        "# ----------------------------\n",
        "cfg = PipelineFactoryConfig(device=device, seg_score_thresh=0.5)\n",
        "pipe = build_default_pipeline(cfg, custom_seg_model=rcnn_model)\n",
        "print(\"Pipeline built with RCNN as segmentation block\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Run a test image\n",
        "# ----------------------------\n",
        "logger = _setup_logger()\n",
        "img = ensure_uint8_rgb(load_rgb(TEST_IMAGE))\n",
        "out = pipe(img)\n",
        "print(f\"Detected {len(out.instance_outputs)} instances in test image {TEST_IMAGE}\")\n",
        "\n",
        "# Display masks overlay\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(img)\n",
        "for i, inst in enumerate(out.instance_outputs[:5]):\n",
        "    plt.imshow(inst.mask, alpha=0.5)\n",
        "plt.title(\"Top-5 RCNN Segmentation Masks Overlay\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FtU7W6kw8mFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}